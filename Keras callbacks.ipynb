{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks\n",
    "\n",
    "This is to show implementations of Keras Callbacks and how to use them\n",
    "\n",
    "- Basic History and plotting  \n",
    "- TensorBoard implementation  \n",
    "- Model checkpoints  \n",
    "- Early Stopping  \n",
    "- Learning Rate Scheduler  \n",
    "- ReduceLROnPlateau  \n",
    "- LambdaCallback\n",
    "- Custom Callback  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.4\n",
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense1 (Dense)               (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,),name='Dense1'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu',name='Dense2'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic History and ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Checkpoints\n",
    "\n",
    "args \n",
    "- filepath - str  can use formating to put epoch number etc {epoc}\n",
    "- monitor - qunatity to monitor  \n",
    "- verbose - 0 or 1  \n",
    "- save_best_only - only save if better than before  \n",
    "- mode - use 'auto'  \n",
    "- save_weights_only - if false then will save whole model\n",
    "- period - the interval between epochs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('./checkpoints/weights_{epoch:02d}_{val_acc:.2f}.hdf5', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Callback\n",
    "\n",
    "args\n",
    "- log_dir : string , must exist\n",
    "- histogram_freq : freq in epochs\n",
    "- write_graph : true or false for seeing the graph in TB  \n",
    "- write_grads :writing the gradients  not working 2.04\n",
    "- batch_size : size of batches for histogram  \n",
    "- write_images :whether to write model weights to see  \n",
    "- embeddings_freq : \n",
    "- embeddings_layer_names :\n",
    "- embeddings_metadata :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TB = TensorBoard(log_dir='./logs', histogram_freq=1,\n",
    "                            write_graph=True,  write_images=False)\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[TB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping - stop training when a monitored quanity has stopped improving\n",
    "args\n",
    "- monitor : what to monitor 'val_loss', 'acc'\n",
    "- min_delta :minimum change to qualify as improvement\n",
    "- patience : number of epocs with no improvement before you stop\n",
    "- verbose : verbosity mode\n",
    "- mode : 'auto' , can be 'min' or 'max' determines direction of improvement. Auto = based on monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=0,verbose=0,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above stopped after 3 epochs instead of 10 as validation loss didn't get better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler\n",
    "arg\n",
    "- schedule - this will be function that takes epoch number(int) and returns new Learning Rate (float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# learning rate schedule for dropping every 10 epochs\n",
    "def LRDropping(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.9\n",
    "    epochs_drop = 3.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LRDropping(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LRDrop = LearningRateScheduler(LRDropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[LRDrop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the learning rate\n",
    "print(K.eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau\n",
    "args\n",
    "- monitor : quality to be monitored eg. 'val_loss' , 'val_acc'\n",
    "- factor : the factor by which the current LR be multiplied  \n",
    "- patience : number of epochs with no improvement  \n",
    "- verbose : 1 = update messages 0 nothing\n",
    "- mode : 'auto'  eg. is improvment up or down 'min' 'max'\n",
    "- epsilon : threshold for measuring the new optimum, to only focus on significant changes  \n",
    "- cooldown :number of epochs to wait before any new changes\n",
    "- min_lr: the lowest lr allowed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_LR = ReduceLROnPlateau(monitor='val_loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[reduce_LR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the learning rate\n",
    "print(K.eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaCallback a way to call anon functions in the callback\n",
    "args\n",
    "- on_epoch_being - called at begin of epoch -takes epoch,logs\n",
    "- on_epoch_end - called at end of epoch -takes epoch,logs\n",
    "- on_batch_begin _ called a begin of a batch -takes batch,logs\n",
    "- on_batch_end :-takes epoch,logs\n",
    "- on_train_begin - -takes logs\n",
    "- on_train_end - -takes logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_print = LambdaCallback(on_epoch_begin=lambda epoch,logs: print(\"lr:\",K.eval(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "lr: 0.000729\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0100 - acc: 0.9979 - val_loss: 0.1215 - val_acc: 0.9855\n",
      "lr: 0.001\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0126 - acc: 0.9973 - val_loss: 0.1440 - val_acc: 0.9832\n",
      "lr: 0.001\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0074 - acc: 0.9983 - val_loss: 0.1456 - val_acc: 0.9829\n",
      "lr: 0.0009\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.1342 - val_acc: 0.9858\n",
      "lr: 0.0009\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0093 - acc: 0.9980 - val_loss: 0.1399 - val_acc: 0.9847\n",
      "lr: 0.0009\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0068 - acc: 0.9985 - val_loss: 0.1404 - val_acc: 0.9839\n",
      "lr: 0.00081\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0086 - acc: 0.9981 - val_loss: 0.1417 - val_acc: 0.9848\n",
      "lr: 0.00081\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0061 - acc: 0.9987 - val_loss: 0.1353 - val_acc: 0.9852\n",
      "lr: 0.00081\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.1460 - val_acc: 0.9850\n",
      "lr: 0.000729\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.0058 - acc: 0.9987 - val_loss: 0.1417 - val_acc: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55329b3940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lr_print,LRDrop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
